Flask쪽 코드를 아래와 같이 정리했다. 함수에서 클래스로 바꿔서 Encapsulation과 추후 확장을 고려하도록 바꿨고, 코드 간의 기능을 분리해서 종속성을 줄였으며 One-class classification 관련 모델들 미리 불러오게 해서 혹시 모를 Latency를 줄였다. 

```python 
import base64
import json
import requests
from joblib import load
import numpy as np
from io import BytesIO
from flask import Flask, request
from flask_classful import FlaskView, route
import firebase_admin
from firebase_admin import credentials
from firebase_admin import storage
from uuid import uuid4
from tensorflow.keras.preprocessing import image
from tensorflow.compat.v1.keras.applications.resnet50 import preprocess_input
import tensorflow as tf
from time import gmtime, strftime


app = Flask(__name__)
standard_scaler = load('scaler.joblib')
pca = load('pca.joblib')
gmm_clf = load('gmm.joblib')
isotonic_reg = load('iso.joblib')

f = open("./project_id.txt", "r")
project_id = f.readline().strip()
cred = credentials.Certificate("./serviceAccountKey.json")
default_app = firebase_admin.initialize_app(cred, {
    "storageBucket": f"{project_id}.appspot.com" 
})       
f.close()


class ClassificationView(FlaskView):
    def index(self):
        return "<h1>Test</h1>"


    def one_class_classify(self, data):
        img = tf.image.resize(data, (224, 224)).numpy()
        one_class_img = preprocess_input(img)
        payload = {
           "instances": one_class_img.tolist()
        }
        request_to_deep_server = requests.post('http://localhost:8891/v1/models/ParrotExtractor:predict', json=payload)
        prediction = json.loads(request_to_deep_server.content.decode('utf-8'))
        one_class_pred = np.array(prediction['predictions'])
        one_pred = standard_scaler.transform(one_class_pred)
        one_pred = pca.transform(one_pred)
        log_probs_test = gmm_clf.score_samples(one_pred)
        test_probabilities = isotonic_reg.predict(log_probs_test)
        test_pred = 1 if test_probabilities[0] >= 0.5 else 0
        if not test_pred:
            return False
        else:
            return True


    def parrot_classify(self, data):
        img_copy = tf.image.resize(data, (299, 299)).numpy()
        img_copy = img_copy/255.
        payload = {
            "instances": [img_copy.tolist()]
        }

        request_to_deep_server = requests.post('http://localhost:8890/v1/models/ParrotClassifier:predict', json=payload)
        pred = json.loads(request_to_deep_server.content.decode('utf-8'))
        return json.dumps(np.array(pred['predictions'])[0].tolist())



    @route("/parrot-classifier/classify", methods=["POST"])
    def classify(self): 
        img = image.img_to_array(image.load_img(BytesIO(base64.b64decode(request.form['b64']))))
        img = img.astype('float16')
        img_copy = np.copy(img)
        img = img[np.newaxis, :]
        is_parrot = self.one_class_classify(img)
        if is_parrot:
            return self.parrot_classify(img_copy)
        else:
            return "Not parrot"


    @route("/parrot-classifier/save", methods=["POST"])
    def save(self):
        img = base64.b64decode(request.form['b64'])
        idx = int(request.form['index'])
        bucket = storage.bucket()
        file_name = f"{idx}/{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}.jpg"
        blob = bucket.blob(file_name)
        new_token = uuid4()
        metadata = {"firebaseStorageDownloadTokens": new_token}
        blob.metadata = metadata
        blob.upload_from_string(data=img, content_type="image/jpeg")
        return "Image save Succeess"


ClassificationView.register(app, route_base = '/')

    
if __name__ == "__main__":
    app.run(port=5000, threaded=False, debug=True)

```

1. 일관된 인터페이스를 위해서 라우팅 URL을 /parrotClassifier/predict/에서 /parrot-classifier/classify로 바꿨다. 카멜케이스 대신에 대쉬를 가운데 넣어 가독성을 높이고 classify 함수 이름을 넣어 컨트롤 자원을 의미하도록 했다. 마지막에 /를 뺐다.  

2. 앱의 결과화면에서 버튼을 클릭하면 앱단에서 파이어베이스 스토리지와 연동하여 이미지를 저장했는데 이 기능을 Flask로 옮겼다. 이 편이 CRUD에 맞는거 같다. 



개선 혹은 추가해야할 사항:

1. 앞으로 더 기능이 추가되면(사용자 로그인, 정보 조회, 업데이트 등) CRUD에 맞는 메소드를 더 추가해야 한다.
2. 지금은 찍은 이미지마다 데이터가 달라서 캐싱할 데이터가 없지만 더 기능이 추가되어 중복 조회 되는 데이터는 캐싱 할 수 있도록 해야한다. 
3. Pagination을 고려한다.
4. 각종 예외처리, 비정상적인 접근에 대한 처리
5. 기능이 추가되면 비동기 프로그래밍 고려(Celery, RabbitMQ를 알아보자)
6. 개발 모드에서 배포 모드 변경



